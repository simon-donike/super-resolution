{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import geopandas\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "import time\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import rescale\n",
    "import cv2\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "from dataloader_SRCNN import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculated dataset file found, loading...\n",
      "\n",
      "Dataset Object successfully instanciated!\n",
      "Loader Length:  68150\n"
     ]
    }
   ],
   "source": [
    "# define paths\n",
    "spot6_mosaic = '/home/simon/CDE_UBS/thesis/data_collection/spot6/spot6_mosaic.tif'\n",
    "spot6_path = \"/home/simon/CDE_UBS/thesis/data_collection/spot6/\"\n",
    "sen2_path = \"/home/simon/CDE_UBS/thesis/data_collection/sen2/merged_reprojected/\"\n",
    "closest_dates_filepath = \"/home/simon/CDE_UBS/thesis/data_loader/data/closest_dates.pkl\"\n",
    "\n",
    "# get dataset object\n",
    "dataset = Dataset(spot6_mosaic,sen2_path,spot6_path,closest_dates_filepath,window_size=500,factor=(10/1.5))\n",
    "loader = DataLoader(dataset,batch_size=1, shuffle=True, num_workers=1)\n",
    "print(\"Loader Length: \",len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    # https://keras.io/examples/vision/super_resolution_sub_pixel/\n",
    "    # https://mfarahmand.medium.com/cnn-based-single-image-super-resolution-6ffcd39ec993\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 // 2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 // 2)\n",
    "        self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding=5 // 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "\"\"\"  \n",
    "def loss_func(a, b):\n",
    "    # PSNR\n",
    "    # https://www.geeksforgeeks.org/python-peak-signal-to-noise-ratio-psnr/#:~:text=Peak%20signal%2Dto%2Dnoise%20ratio%20(PSNR)%20is%20the,with%20the%20maximum%20possible%20power.\n",
    "    # PSNR = 20*log(max(max(f)))/((MSE)^0.5)\n",
    "    # result in db\n",
    "    # lower->better\n",
    "    try:\n",
    "        import math\n",
    "        mse = np.mean((a - b) ** 2)\n",
    "        if(mse == 0):  # MSE is zero means no noise is present in the signal. Therefore PSNR have no importance.\n",
    "            return 100\n",
    "        max_pixel = 255.0\n",
    "        psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
    "        return(psnr)\n",
    "    except ValueError:\n",
    "        return(0)\n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def loss_func(a,b):\n",
    "    # MAE\n",
    "    error = torch.abs(a - b).sum().data\n",
    "    error = Variable(error.data, requires_grad=True)\n",
    "    return(error)\n",
    "\"\"\"\n",
    "\n",
    "def loss_func(a,b):\n",
    "    # MSE\n",
    "    loss = nn.MSELoss()\n",
    "    mse = loss(a, b)\n",
    "    mse = Variable(mse.data, requires_grad=True)\n",
    "    return(mse)\n",
    "    \n",
    "model = SRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of model trainer function\n",
    "def train_model(model,batch_size=1,lr=0.01,epochs=10,wandb_name=\"test\"):\n",
    "    \n",
    "    logging=True\n",
    "    if logging==True:\n",
    "        wandb.init(project=wandb_name, entity=\"simon-donike\")\n",
    "        wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epochs,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    \n",
    "    # define loaders\n",
    "    loader_train = DataLoader(dataset,batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    loader_test  = DataLoader(dataset,batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    loader_full  = DataLoader(dataset,batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "    train_loss = []  # where we keep track of the training loss\n",
    "    train_accuracy = []  # where we keep track of the training accuracy of the model\n",
    "    val_loss = []  # where we keep track of the validation loss\n",
    "    val_accuracy = []  # where we keep track of the validation accuracy of the model\n",
    "    epochs = epochs  # number of epochs\n",
    "\n",
    "    # initialize model\n",
    "\n",
    "    model = model.double()\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        for (x_train_batch, y_train_batch) in loader_train:\n",
    "            x_train_batch = x_train_batch.to(torch.double)\n",
    "            x_train_batch = x_train_batch.to(device)\n",
    "\n",
    "            y_train_batch = y_train_batch.to(torch.double) \n",
    "            y_train_batch = y_train_batch.to(device)\n",
    "            y_hat = model(x_train_batch)  # forward pass\n",
    "\n",
    "            loss = loss_func(y_hat, y_train_batch)  # compute the loss\n",
    "\n",
    "            loss.backward()  # obtain the gradients with respect to the loss\n",
    "            optimizer.step()  # perform one step of gradient descent\n",
    "            optimizer.zero_grad()  # reset the gradients to 0\n",
    "            y_hat_class = torch.argmax(y_hat.detach(), axis=1)  # we assign an appropriate label based on the network's prediction\n",
    "            train_correct += torch.sum(y_hat_class == y_train_batch)\n",
    "            train_loss.append(loss.item() / len(x_train_batch))\n",
    "            if logging==True:\n",
    "                wandb.log({'loss': loss.item() / len(x_train_batch)})\n",
    "\n",
    "        train_accuracy.append(train_correct / len(loader_train.dataset))\n",
    "        if logging==True:\n",
    "            wandb.log({'train_acc': train_correct / len(loader_train.dataset)})\n",
    "\n",
    "\n",
    "        print ('Epoch', e+1, ' finished.')\n",
    "\n",
    "    if logging==True:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon-donike\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/simon-donike/SRCNN/runs/1ozro8d2\" target=\"_blank\">usual-lion-30</a></strong> to <a href=\"https://wandb.ai/simon-donike/SRCNN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/CDE_UBS/thesis/testing/dataloader_SRCNN.py:296: UserWarning: WARNING: More image aqc. dates requested than available. Recalculate full validity dataframe or request fewer sen2 images!\n",
      "  warnings.warn(\"WARNING: More image aqc. dates requested than available. Recalculate full validity dataframe or request fewer sen2 images!\")\n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "Caught RasterioIOError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 261, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_shim.pyx\", line 78, in rasterio._shim.open_dataset\n  File \"rasterio/_err.pyx\", line 216, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: E: No such file or directory\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/anaconda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/anaconda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/anaconda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/simon/CDE_UBS/thesis/testing/dataloader_SRCNN.py\", line 132, in __getitem__\n    im_sen2 = Dataset.extract_sen2_window(Dataset.get_valid_sen2paths(current_dict,sen2_path),current_coor,self.window_size_sen2)\n  File \"/home/simon/CDE_UBS/thesis/testing/dataloader_SRCNN.py\", line 260, in extract_sen2_window\n    with rasterio.open(file_path) as dataset:\n  File \"/home/simon/.local/lib/python3.7/site-packages/rasterio/env.py\", line 437, in wrapper\n    return f(*args, **kwds)\n  File \"/home/simon/.local/lib/python3.7/site-packages/rasterio/__init__.py\", line 220, in open\n    s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 263, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: E: No such file or directory\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c9575de4139c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwandb_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SRCNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-4b7161b777ac>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, batch_size, lr, epochs, wandb_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mx_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mx_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: Caught RasterioIOError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 261, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_shim.pyx\", line 78, in rasterio._shim.open_dataset\n  File \"rasterio/_err.pyx\", line 216, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: E: No such file or directory\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/anaconda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/anaconda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/anaconda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/simon/CDE_UBS/thesis/testing/dataloader_SRCNN.py\", line 132, in __getitem__\n    im_sen2 = Dataset.extract_sen2_window(Dataset.get_valid_sen2paths(current_dict,sen2_path),current_coor,self.window_size_sen2)\n  File \"/home/simon/CDE_UBS/thesis/testing/dataloader_SRCNN.py\", line 260, in extract_sen2_window\n    with rasterio.open(file_path) as dataset:\n  File \"/home/simon/.local/lib/python3.7/site-packages/rasterio/env.py\", line 437, in wrapper\n    return f(*args, **kwds)\n  File \"/home/simon/.local/lib/python3.7/site-packages/rasterio/__init__.py\", line 220, in open\n    s = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 263, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: E: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "train_model(model=model,batch_size=1,lr=0.01,epochs=25,wandb_name=\"SRCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
