{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import geopandas\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "import time\n",
    "\n",
    "import skimage\n",
    "from skimage.transform import rescale\n",
    "import cv2\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local imports\n",
    "from dataloader_SRCNN import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculated dataset file found, loading...\n",
      "\n",
      "Dataset Object successfully instanciated!\n",
      "Loader Length:  68150\n"
     ]
    }
   ],
   "source": [
    "# define paths\n",
    "spot6_mosaic = '/home/simon/CDE_UBS/thesis/data_collection/spot6/spot6_mosaic.tif'\n",
    "spot6_path = \"/home/simon/CDE_UBS/thesis/data_collection/spot6/\"\n",
    "sen2_path = \"/home/simon/CDE_UBS/thesis/data_collection/sen2/merged_reprojected/\"\n",
    "closest_dates_filepath = \"/home/simon/CDE_UBS/thesis/data_loader/data/closest_dates.pkl\"\n",
    "\n",
    "# get dataset object\n",
    "dataset = Dataset(spot6_mosaic,sen2_path,spot6_path,closest_dates_filepath,window_size=500,factor=(10/1.5))\n",
    "loader = DataLoader(dataset,batch_size=1, shuffle=True, num_workers=1)\n",
    "print(\"Loader Length: \",len(loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 500\n",
    "upscale_factor = 6.6666\n",
    "input_size = crop_size // upscale_factor\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    # https://keras.io/examples/vision/super_resolution_sub_pixel/\n",
    "    def __init__(self, num_channels=3):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 // 2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 // 2)\n",
    "        self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding=5 // 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "\"\"\"  \n",
    "def loss_func(a, b):\n",
    "    # PSNR\n",
    "    # https://www.geeksforgeeks.org/python-peak-signal-to-noise-ratio-psnr/#:~:text=Peak%20signal%2Dto%2Dnoise%20ratio%20(PSNR)%20is%20the,with%20the%20maximum%20possible%20power.\n",
    "    # PSNR = 20*log(max(max(f)))/((MSE)^0.5)\n",
    "    # result in db\n",
    "    # lower->better\n",
    "    try:\n",
    "        import math\n",
    "        mse = np.mean((a - b) ** 2)\n",
    "        if(mse == 0):  # MSE is zero means no noise is present in the signal. Therefore PSNR have no importance.\n",
    "            return 100\n",
    "        max_pixel = 255.0\n",
    "        psnr = 20 * math.log10(max_pixel / math.sqrt(mse))\n",
    "        return(psnr)\n",
    "    except ValueError:\n",
    "        return(0)\n",
    "\n",
    "    \n",
    "def loss_func(a,b):\n",
    "    mse = nn.MSELoss(a,b)\n",
    "    return(mse)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def loss_func(a,b):\n",
    "    error = torch.abs(a - b).sum().data\n",
    "    error = Variable(error.data, requires_grad=True)\n",
    "    return(error)\n",
    "    \n",
    "model = SRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of model trainer function\n",
    "def train_model(model,batch_size=1,lr=0.01,epochs=10,wandb_name=\"test\"):\n",
    "    \n",
    "    logging=True\n",
    "    if logging==True:\n",
    "        wandb.init(project=wandb_name, entity=\"simon-donike\")\n",
    "        wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epochs,\n",
    "          \"batch_size\": batch_size\n",
    "        }\n",
    "    \n",
    "    # define loaders\n",
    "    loader_train = DataLoader(dataset,batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    loader_test  = DataLoader(dataset,batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    loader_full  = DataLoader(dataset,batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "\n",
    "    train_loss = []  # where we keep track of the training loss\n",
    "    train_accuracy = []  # where we keep track of the training accuracy of the model\n",
    "    val_loss = []  # where we keep track of the validation loss\n",
    "    val_accuracy = []  # where we keep track of the validation accuracy of the model\n",
    "    epochs = epochs  # number of epochs\n",
    "\n",
    "    # initialize model\n",
    "\n",
    "    model = model.double()\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        for (x_train_batch, y_train_batch) in loader_train:\n",
    "            x_train_batch = x_train_batch.to(torch.double)\n",
    "            x_train_batch = x_train_batch.to(device)\n",
    "\n",
    "            y_train_batch = y_train_batch.to(torch.double) \n",
    "            y_train_batch = y_train_batch.to(device)\n",
    "            y_hat = model(x_train_batch)  # forward pass\n",
    "\n",
    "            loss = loss_func(y_hat, y_train_batch)  # compute the loss\n",
    "\n",
    "            loss.backward()  # obtain the gradients with respect to the loss\n",
    "            optimizer.step()  # perform one step of gradient descent\n",
    "            optimizer.zero_grad()  # reset the gradients to 0\n",
    "            y_hat_class = torch.argmax(y_hat.detach(), axis=1)  # we assign an appropriate label based on the network's prediction\n",
    "            train_correct += torch.sum(y_hat_class == y_train_batch)\n",
    "            train_loss.append(loss.item() / len(x_train_batch))\n",
    "            if logging==True:\n",
    "                wandb.log({'loss': loss.item() / len(x_train_batch)})\n",
    "\n",
    "        train_accuracy.append(train_correct / len(loader_train.dataset))\n",
    "        if logging==True:\n",
    "            wandb.log({'train_acc': train_correct / len(loader_train.dataset)})\n",
    "\n",
    "\n",
    "        print ('Epoch', e+1, ' finished.')\n",
    "\n",
    "    if logging==True:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon-donike\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/simon-donike/SRCNN/runs/2gbm9xh7\" target=\"_blank\">fresh-cloud-11</a></strong> to <a href=\"https://wandb.ai/simon-donike/SRCNN\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(model=model,batch_size=1,lr=0.01,epochs=25,wandb_name=\"SRCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
