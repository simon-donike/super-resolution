{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6855b308-6f3b-48d4-9bdb-9628a232bd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\accou\\anaconda3\\envs\\geo_env_n6\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from models import Generator, Discriminator, TruncatedVGG19\n",
    "from datasets import SRDataset\n",
    "from utils import *\n",
    "from utils_.dataloader import Dataset as dataset\n",
    "import utils_.helper_functions as helper_functions\n",
    "from utils_.losses import calculate_metrics\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from pynvml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82835983-b615-4334-aca9-c90634e77cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "data_folder = './'  # folder with JSON data files\n",
    "crop_size = 300  # crop size of target HR images\n",
    "scaling_factor = 4  # the scaling factor for the generator; the input LR images will be downsampled from the target HR images by this factor\n",
    "\n",
    "# Generator parameters\n",
    "large_kernel_size_g = 9  # kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "small_kernel_size_g = 3  # kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "n_channels_g = 64  # number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "n_blocks_g = 16  # number of residual blocks\n",
    "srresnet_checkpoint = None  # filepath of the trained SRResNet checkpoint used for initialization\n",
    "\n",
    "# Discriminator parameters\n",
    "kernel_size_d = 3  # kernel size in all convolutional blocks\n",
    "n_channels_d = 64 # CHANGED FROM 64 for temporal inclusion!!!!!  # number of output channels in the first convolutional block, after which it is doubled in every 2nd block thereafter\n",
    "n_blocks_d = 8  # number of convolutional blocks\n",
    "fc_size_d = 1024  # size of the first fully connected layer\n",
    "\n",
    "# Learning parameters\n",
    "checkpoint = None # None  # path to model (SRGAN) checkpoint, None if none\n",
    "batch_size = 2  # batch size\n",
    "start_epoch = 0  # start at this epoch\n",
    "iterations = 2e10  # number of training iterations\n",
    "workers = 0  # number of workers for loading data in the DataLoader\n",
    "vgg19_i = 5  # the index i in the definition for VGG loss; see paper or models.py\n",
    "vgg19_j = 4  # the index j in the definition for VGG loss; see paper or models.py\n",
    "beta = 1e-3  # the coefficient to weight the adversarial loss in the perceptual loss\n",
    "print_freq = 1000  # print training status once every __ batches\n",
    "lr = 1e-4  # learning rate 1e-4\n",
    "grad_clip = None  # clip if gradients are exploding\n",
    "\n",
    "# Default device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "logging=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb8014-4b3e-4a28-9115-82d42f60be42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db0ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3192964-6cd8-4d5b-a999-41c1c97a2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training.\n",
    "    \"\"\"\n",
    "    global start_epoch, epoch, checkpoint, srresnet_checkpoint\n",
    "\n",
    "    # Initialize model or load checkpoint\n",
    "    if checkpoint is None:\n",
    "        # Generator\n",
    "        generator = Generator(large_kernel_size=large_kernel_size_g,\n",
    "                              small_kernel_size=small_kernel_size_g,\n",
    "                              n_channels=n_channels_g,\n",
    "                              n_blocks=n_blocks_g,\n",
    "                              scaling_factor=scaling_factor)\n",
    "\n",
    "        # Initialize generator network with pretrained SRResNet\n",
    "        try:\n",
    "            generator.initialize_with_srresnet(srresnet_checkpoint=srresnet_checkpoint)\n",
    "            print(\"pretrained model loaded:\",srresnet_checkpoint)\n",
    "        except:\n",
    "            print(\"Error occured, pretrained generator SrresNet not loaded.\")\n",
    "\n",
    "        # Initialize generator's optimizer\n",
    "        optimizer_g = torch.optim.Adam(params=filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "                                       lr=lr)\n",
    "\n",
    "        # Discriminator\n",
    "        discriminator = Discriminator(kernel_size=kernel_size_d,\n",
    "                                      n_channels=n_channels_d,\n",
    "                                      n_blocks=n_blocks_d,\n",
    "                                      fc_size=fc_size_d)\n",
    "\n",
    "        # Initialize discriminator's optimizer\n",
    "        optimizer_d = torch.optim.Adam(params=filter(lambda p: p.requires_grad, discriminator.parameters()),\n",
    "                                       lr=lr)\n",
    "        \n",
    "        try:\n",
    "            #preloaded = torch.load(\"checkpoints\\\\checkpoint_srgan22-04-2022_03-52-18.pth.tar\")\n",
    "            #generator = preloaded[\"generator\"] \n",
    "            #discriminator = preloaded[\"discriminator\"]\n",
    "            \n",
    "            load_from = \".\"#\"C:\\\\Users\\\\accou\\\\Documents\\\\GitHub\\\\a-PyTorch-Tutorial-to-Super-Resolution\\\\checkpoints\\\\faulty_data_SRGAN\\\\checkpoint_SRGAN_25-04-2022_20-12-39_epoch96_BEST.pth.tar\"\n",
    "            generator = torch.load(load_from)[\"generator\"]\n",
    "            discriminator = torch.load(load_from)[\"discriminator\"]\n",
    "            optimizer_g = torch.load(load_from)[\"optimizer_g\"]\n",
    "            optimzer_d = torch.load(load_from)[\"optimizer_d\"]\n",
    "            print(\"Checkpoint loaded!\")\n",
    "        except:\n",
    "            print(\"error occured, pretrained generator/descriminator not laoded!\")\n",
    "        \n",
    "    else:\n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint)\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            generator = checkpoint['generator']\n",
    "            discriminator = checkpoint['discriminator']\n",
    "            optimizer_g = checkpoint['optimizer_g']\n",
    "            optimizer_d = checkpoint['optimizer_d']\n",
    "            print(\"\\nLoaded checkpoint from epoch %d.\\n\" % (checkpoint['epoch'] + 1))\n",
    "        except:\n",
    "            generator = Generator(large_kernel_size=large_kernel_size_g,\n",
    "                              small_kernel_size=small_kernel_size_g,\n",
    "                              n_channels=n_channels_g,\n",
    "                              n_blocks=n_blocks_g,\n",
    "                              scaling_factor=scaling_factor)\n",
    "            print(\"Error occured, model not loaded. Training from scratch...\")\n",
    "\n",
    "    # Truncated VGG19 network to be used in the loss calculation\n",
    "    truncated_vgg19 = TruncatedVGG19(i=vgg19_i, j=vgg19_j)\n",
    "    truncated_vgg19.eval()\n",
    "\n",
    "    # Loss functions\n",
    "    content_loss_criterion = nn.MSELoss()\n",
    "    adversarial_loss_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Move to default device\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    truncated_vgg19 = truncated_vgg19.to(device)\n",
    "    content_loss_criterion = content_loss_criterion.to(device)\n",
    "    adversarial_loss_criterion = adversarial_loss_criterion.to(device)\n",
    "\n",
    "\n",
    "    # Custom dataloaders\n",
    "    working_directory = \"C:\\\\Users\\\\accou\\\\Documents\\\\GitHub\\\\a-PyTorch-Tutorial-to-Super-Resolution_FUSION\\\\\"\n",
    "    folder_path = \"C:\\\\Users\\\\accou\\\\Documents\\\\thesis\\\\data_v2\\\\\"\n",
    "    dataset_file = \"C:\\\\Users\\\\accou\\\\Documents\\\\thesis\\\\data_v2\\\\final_dataset.pkl\"\n",
    "    transform = \"histogram_matching\"\n",
    "    sen2_tile_train = \"all\"\n",
    "    sen2_tile_test   = \"all\"\n",
    "    sen2_tile_val  = \"all\"\n",
    "    location = \"local\"\n",
    "    \n",
    "    dataset_train = dataset(folder_path,dataset_file,test_train_val=\"train\",transform=transform,sen2_amount=4, sen2_tile = sen2_tile_train, location=location)\n",
    "    train_loader = DataLoader(dataset_train,batch_size=batch_size, shuffle=True, num_workers=1,pin_memory=True,drop_last=True)\n",
    "    \n",
    "    dataset_test = dataset(folder_path,dataset_file,test_train_val=\"test\",transform=transform,sen2_amount=4, sen2_tile = sen2_tile_test, location=location)\n",
    "    test_loader = DataLoader(dataset_test,batch_size=2, shuffle=True, num_workers=1,pin_memory=True,drop_last=True)\n",
    "    \n",
    "    print(\"dataloader instanciated!\")\n",
    "    print(\"Len. Train: \",len(train_loader),\"(Batch Sz. \"+str(batch_size),\")\")\n",
    "    \n",
    "    # Total number of epochs to train for\n",
    "    #epochs = int(iterations // len(train_loader) + 1)\n",
    "    epochs = 2000\n",
    "\n",
    "    if logging==True:\n",
    "        run_name = \"SRGAN_Fusion_\"+str(datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
    "        wandb.init(name=run_name,project=\"SRGAN_Fusion\", entity=\"simon-donike\")\n",
    "        wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epochs,\n",
    "          \"batch_size\": batch_size}\n",
    "    \n",
    "    \n",
    "    # Epochs\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        wandb.log({'epoch': epoch})\n",
    "        # At the halfway point, reduce learning rate to a tenth\n",
    "        if epoch == int((iterations / 2) // len(train_loader) + 1):\n",
    "            adjust_learning_rate(optimizer_g, 0.1)\n",
    "            adjust_learning_rate(optimizer_d, 0.1)\n",
    "\n",
    "        # One epoch's training\n",
    "        \n",
    "        train(train_loader=train_loader,\n",
    "              test_loader=test_loader,\n",
    "              generator=generator,\n",
    "              discriminator=discriminator,\n",
    "              truncated_vgg19=truncated_vgg19,\n",
    "              content_loss_criterion=content_loss_criterion,\n",
    "              adversarial_loss_criterion=adversarial_loss_criterion,\n",
    "              optimizer_g=optimizer_g,\n",
    "              optimizer_d=optimizer_d,\n",
    "              epoch=epoch)\n",
    "        \n",
    "        \n",
    "        # log test metrics\n",
    "        try:\n",
    "            test(generator,test_loader)\n",
    "        except:\n",
    "            print(\"End of epoch Test failed, no problem for process. Test is calculated every x batches anyways.\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'generator': generator,\n",
    "                    'discriminator': discriminator,\n",
    "                    'optimizer_g': optimizer_g,\n",
    "                    'optimizer_d': optimizer_d},\n",
    "                    'checkpoints//checkpoint_SRGAN_'+str(datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"))+'_epoch'+str(epoch)+'.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76abec05-93aa-4cf3-85e2-2a69dc2db4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(generator,test_loader,amount=10):\n",
    "    import utils_.losses as losses\n",
    "    # keep track of error metrics\n",
    "    ssim_ls  = []\n",
    "    psnr_ls  = []\n",
    "    lpips_ls = []\n",
    "    mae_ls = []\n",
    "    ssim_int_ls  = []\n",
    "    psnr_int_ls  = []\n",
    "    lpips_int_ls = []\n",
    "    mae_int_ls = []\n",
    "    \n",
    "    # perform 10 predictions, append results to list\n",
    "    for i in range(0,amount):\n",
    "        lr,hr = next(iter(test_loader))\n",
    "        lr,hr = lr.to(device),hr.to(device)\n",
    "        sr = generator(lr)\n",
    "        sr = sr[0].unsqueeze(0)\n",
    "        hr = hr[0].unsqueeze(0)\n",
    "        l = losses.calculate_metrics(hr,lr,sr)\n",
    "        \n",
    "        \n",
    "        lpips_ls.append(l[0])\n",
    "        psnr_ls.append(l[1])\n",
    "        ssim_ls.append(l[2])\n",
    "        mae_ls.append(l[3])\n",
    "        lpips_int_ls.append(l[4])\n",
    "        psnr_int_ls.append(l[5])\n",
    "        ssim_int_ls.append(l[6])\n",
    "        mae_int_ls.append(l[6])\n",
    "    \n",
    "    def Average(lst):\n",
    "        return sum(lst) / len(lst)\n",
    "    \n",
    "    ssim_avg  = round(Average(ssim_ls),5)\n",
    "    psnr_avg  = round(Average(psnr_ls),5)\n",
    "    lpips_avg = round(Average(lpips_ls),5)\n",
    "    mae_avg = round(Average(mae_ls),5)\n",
    "    ssim_int_avg  = round(Average(ssim_int_ls),5)\n",
    "    psnr_int_avg  = round(Average(psnr_int_ls),5)\n",
    "    lpips_int_avg = round(Average(lpips_int_ls),5)\n",
    "    mae_int_avg = round(Average(mae_int_ls),5)\n",
    "    \n",
    "    wandb.log({'epoch_test_psnr': psnr_avg,\n",
    "               'epoch_test_ssim': ssim_avg,\n",
    "               'epoch_test_lpips': lpips_avg,\n",
    "               'epoch_test_mae': mae_avg})\n",
    "    \n",
    "    #return([lpips_avg, psnr_avg, ssim_avg, mae_avg, lpips_int_avg, psnr_int_avg, ssim_int_avg, mae_int_avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10f3c56-c7b3-4a44-bb39-e2de8b408145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram():\n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    print(f'total    : {info.total*0.000000001}')\n",
    "    print(f'free     : {info.free*0.000000001}')\n",
    "    print(f'used     : {info.used*0.000000001}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32511e21-6a44-4175-8f5c-47cf6a0659cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,test_loader, generator, discriminator, truncated_vgg19, content_loss_criterion, adversarial_loss_criterion,\n",
    "          optimizer_g, optimizer_d, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "\n",
    "    :param train_loader: train dataloader\n",
    "    :param generator: generator\n",
    "    :param discriminator: discriminator\n",
    "    :param truncated_vgg19: truncated VGG19 network\n",
    "    :param content_loss_criterion: content loss function (Mean Squared-Error loss)\n",
    "    :param adversarial_loss_criterion: adversarial loss function (Binary Cross-Entropy loss)\n",
    "    :param optimizer_g: optimizer for the generator\n",
    "    :param optimizer_d: optimizer for the discriminator\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    # Set to train mode\n",
    "    generator.train()\n",
    "    discriminator.train()  # training mode enables batch normalization\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses_c = AverageMeter()  # content loss\n",
    "    losses_a = AverageMeter()  # adversarial loss in the generator\n",
    "    losses_d = AverageMeter()  # adversarial loss in the discriminator\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    \n",
    "    # Batches\n",
    "    for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        \n",
    "        while lr_imgs.shape!=torch.Size([2,12,75,75]):\n",
    "            lr_imgs,hr_imgs = next(iter(train_loader))\n",
    "            print(\"LAST RESORT - ERROR: shape invalid, loaded next image in train\")\n",
    "        \n",
    "        # Move to default device\n",
    "        lr_imgs = lr_imgs.to(device)  # (batch_size (N), 3,  75,  75), imagenet-normed\n",
    "        hr_imgs = hr_imgs.to(device)  # (batch_size (N), 3, 300, 300), imagenet-normed\n",
    "\n",
    "        \n",
    "        # GENERATOR UPDATE\n",
    "        \n",
    "        # EATS about 4gb VRAM\n",
    "        # Generate\n",
    "        sr_imgs = generator(lr_imgs)  # (N, 3, 300, 300), in [-1, 1]\n",
    "        sr_imgs = convert_image(sr_imgs, source='[-1, 1]', target='imagenet-norm')  # (N, 3, 96, 96), imagenet-normed\n",
    "        \n",
    "        \n",
    "        # EATS 6gb VRAM\n",
    "        # Calculate VGG feature maps for the super-resolved (SR) and high resolution (HR) images\n",
    "        sr_imgs_in_vgg_space = truncated_vgg19(sr_imgs)\n",
    "        hr_imgs_in_vgg_space = truncated_vgg19(hr_imgs).detach()  # detached because they're constant, targets\n",
    "        \n",
    "\n",
    "        # Calculate the Perceptual loss\n",
    "        content_loss = content_loss_criterion(sr_imgs_in_vgg_space, hr_imgs_in_vgg_space)\n",
    "        del sr_imgs_in_vgg_space,hr_imgs_in_vgg_space\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        # EATS like 4gb VRAM\n",
    "        # Discriminate super-resolved (SR) images\n",
    "        sr_discriminated = discriminator(sr_imgs)  # (N)\n",
    "        adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.ones_like(sr_discriminated))\n",
    "\n",
    "        \n",
    "        perceptual_loss = content_loss + beta * adversarial_loss\n",
    "        wandb.log({'content_loss': content_loss,\n",
    "                   'adversarial_loss':adversarial_loss,\n",
    "                   'perceptual_loss':perceptual_loss})\n",
    "    \n",
    "        # Back-prop.\n",
    "        optimizer_g.zero_grad()\n",
    "        perceptual_loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer_g, grad_clip)\n",
    "\n",
    "        # Update generator\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Keep track of loss\n",
    "        losses_c.update(content_loss.item(), lr_imgs.size(0))\n",
    "        losses_a.update(adversarial_loss.item(), lr_imgs.size(0))\n",
    "\n",
    "        # DISCRIMINATOR UPDATE\n",
    "        # Discriminate super-resolution (SR) and high-resolution (HR) images\n",
    "        hr_discriminated = discriminator(hr_imgs)\n",
    "        sr_discriminated = discriminator(sr_imgs.detach())\n",
    "\n",
    "        \n",
    "        wandb.log({'discr_hr': float(torch.mean(hr_discriminated).item()),\n",
    "                   'discr_sr': float(torch.mean(sr_discriminated).item())})\n",
    "\n",
    "        # But didn't we already discriminate the SR images earlier, before updating the generator (G)? Why not just use that here?\n",
    "        # Because, if we used that, we'd be back-propagating (finding gradients) over the G too when backward() is called\n",
    "        # It's actually faster to detach the SR images from the G and forward-prop again, than to back-prop. over the G unnecessarily\n",
    "        # See FAQ section in the tutorial\n",
    "\n",
    "        # Binary Cross-Entropy loss\n",
    "        adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.zeros_like(sr_discriminated)) + adversarial_loss_criterion(hr_discriminated, torch.ones_like(hr_discriminated))\n",
    "        del sr_discriminated, hr_discriminated\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Back-prop.\n",
    "        optimizer_d.zero_grad()\n",
    "        adversarial_loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer_d, grad_clip)\n",
    "\n",
    "        # Update discriminator\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Keep track of loss\n",
    "        losses_d.update(adversarial_loss.item(), hr_imgs.size(0))\n",
    "\n",
    "        # Keep track of batch times\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        # Reset start time\n",
    "        start = time.time()\n",
    "        #print(\"hr\",hr_imgs.shape)\n",
    "        #print(\"hr\",lr_imgs.shape)\n",
    "        #print(\"sr\",sr_imgs.shape)\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            test(generator,test_loader)\n",
    "            helper_functions.plot_tensors_window(hr_imgs,lr_imgs[:,:3,:,:],sr_imgs,fig_path=\"C:\\\\Users\\\\accou\\\\Documents\\\\GitHub\\\\a-PyTorch-Tutorial-to-Super-Resolution_FUSION\\\\images_sat\\\\\")\n",
    "            print('Epoch: [{0}][{1}/{2}]----'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})----'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})----'\n",
    "                  'Cont. Loss {loss_c.val:.4f} ({loss_c.avg:.4f})----'\n",
    "                  'Adv. Loss {loss_a.val:.4f} ({loss_a.avg:.4f})----'\n",
    "                  'Disc. Loss {loss_d.val:.4f} ({loss_d.avg:.4f})'.format(epoch,\n",
    "                                                                          i,\n",
    "                                                                          len(train_loader),\n",
    "                                                                          batch_time=batch_time,\n",
    "                                                                          data_time=data_time,\n",
    "                                                                          loss_c=losses_c,\n",
    "                                                                          loss_a=losses_a,\n",
    "                                                                          loss_d=losses_d))\n",
    "\n",
    "    del lr_imgs, hr_imgs, sr_imgs   # free some memory since their histories may be stored\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4096588-2a29-4af0-8467-165ffe3fef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b604471-9e9a-403d-a0fc-d4b2a36f6b35",
   "metadata": {},
   "source": [
    "# SAVE PTH AS WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "140dc2ea-e60c-4469-9059-002e50069a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    from models import Generator\n",
    "    # Generator parameters\n",
    "    large_kernel_size_g = 9  # kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "    small_kernel_size_g = 3  # kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "    n_channels_g = 64  # number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "    n_blocks_g = 16  # number of residual blocks\n",
    "    scaling_factor = 4\n",
    "    generator = Generator(large_kernel_size=large_kernel_size_g,\n",
    "                                  small_kernel_size=small_kernel_size_g,\n",
    "                                  n_channels=n_channels_g,\n",
    "                                  n_blocks=n_blocks_g,\n",
    "                                  scaling_factor=scaling_factor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08682aa1-592a-4be7-9167-681c81049978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#a = torch.load(\"checkpoints/conversion/srgan_fusion.tar\")\n",
    "#torch.save(a.state_dict(), 'checkpoints/conversion/fusion_model_weights.pth')\n",
    "model = torch.load(\"checkpoints/checkpoint_SRGAN_10-06-2022_13-30-51_epoch77.pth.tar\")[\"generator\"]\n",
    "torch.save(model.state_dict(), 'checkpoints/conversion/fusion_model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14763f2a-f5bb-4c24-bedf-767caf29b701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac2d21-cceb-43d1-8dea-d5a90e23c5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d61648-129b-4864-a9f3-ceb49d808d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971dc6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
