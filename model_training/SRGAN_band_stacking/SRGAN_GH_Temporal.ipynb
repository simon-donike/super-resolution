{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6855b308-6f3b-48d4-9bdb-9628a232bd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\accou\\anaconda3\\envs\\geo_env3\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from models import Generator, Discriminator, TruncatedVGG19\n",
    "from datasets import SRDataset\n",
    "from utils import *\n",
    "from utils_.dataloader import Dataset as dataset\n",
    "import utils_.helper_functions as helper_functions\n",
    "from utils_.losses import calculate_metrics\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from pynvml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7ff095-d7ca-4466-83a8-6d75b60c8de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#a = torch.rand(25,16,1000,1000)\n",
    "#a = a.to(device)\n",
    "#time.sleep(5)\n",
    "#del a\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82835983-b615-4334-aca9-c90634e77cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Data parameters\n",
    "data_folder = './'  # folder with JSON data files\n",
    "crop_size = 300  # crop size of target HR images\n",
    "scaling_factor = 4  # the scaling factor for the generator; the input LR images will be downsampled from the target HR images by this factor\n",
    "sen2_amount = 4 # number of sen2 images to be loaded\n",
    "\n",
    "\n",
    "# Generator parameters\n",
    "large_kernel_size_g = 9  # kernel size of the first and last convolutions which transform the inputs and outputs\n",
    "small_kernel_size_g = 3  # kernel size of all convolutions in-between, i.e. those in the residual and subpixel convolutional blocks\n",
    "n_channels_g = 64  # number of channels in-between, i.e. the input and output channels for the residual and subpixel convolutional blocks\n",
    "n_blocks_g = 16  # number of residual blocks\n",
    "srresnet_checkpoint = None  # filepath of the trained SRResNet checkpoint used for initialization\n",
    "\n",
    "# Discriminator parameters\n",
    "kernel_size_d = 3  # kernel size in all convolutional blocks\n",
    "n_channels_d = 64  # number of output channels in the first convolutional block, after which it is doubled in every 2nd block thereafter\n",
    "n_blocks_d = 8  # number of convolutional blocks\n",
    "fc_size_d = 1024  # size of the first fully connected layer\n",
    "input_channels = 3*sen2_amount\n",
    "\n",
    "# Learning parameters\n",
    "checkpoint = None # None  # path to model (SRGAN) checkpoint, None if none\n",
    "batch_size = 6  # batch size\n",
    "start_epoch = 0  # start at this epoch\n",
    "iterations = 2e5  # number of training iterations\n",
    "workers = 0  # number of workers for loading data in the DataLoader\n",
    "vgg19_i = 5  # the index i in the definition for VGG loss; see paper or models.py\n",
    "vgg19_j = 4  # the index j in the definition for VGG loss; see paper or models.py\n",
    "beta = 1e-3  # the coefficient to weight the adversarial loss in the perceptual loss\n",
    "print_freq = 250  # print training status once every __ batches\n",
    "lr = 1e-4  # learning rate 1e-4\n",
    "grad_clip = None  # clip if gradients are exploding\n",
    "\n",
    "# Default device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "logging=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3192964-6cd8-4d5b-a999-41c1c97a2ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Training.\n",
    "    \"\"\"\n",
    "    global start_epoch, epoch, checkpoint, srresnet_checkpoint\n",
    "\n",
    "    if checkpoint==None:\n",
    "        # Generator\n",
    "        generator = Generator(large_kernel_size=large_kernel_size_g,\n",
    "                              small_kernel_size=small_kernel_size_g,\n",
    "                              n_channels=n_channels_g,\n",
    "                              n_blocks=n_blocks_g,\n",
    "                              scaling_factor=scaling_factor,\n",
    "                             input_channels=input_channels)\n",
    "\n",
    "        # Initialize generator's optimizer\n",
    "        optimizer_g = torch.optim.Adam(params=filter(lambda p: p.requires_grad, generator.parameters()),\n",
    "                                       lr=lr)\n",
    "\n",
    "        # Discriminator\n",
    "        discriminator = Discriminator(kernel_size=kernel_size_d,\n",
    "                                      n_channels=n_channels_d,\n",
    "                                      n_blocks=n_blocks_d,\n",
    "                                      fc_size=fc_size_d)\n",
    "\n",
    "        # Initialize discriminator's optimizer\n",
    "        optimizer_d = torch.optim.Adam(params=filter(lambda p: p.requires_grad, discriminator.parameters()),\n",
    "                                       lr=lr)\n",
    "        \n",
    "    # Truncated VGG19 network to be used in the loss calculation\n",
    "    truncated_vgg19 = TruncatedVGG19(i=vgg19_i, j=vgg19_j)\n",
    "    truncated_vgg19.eval()\n",
    "\n",
    "    # Loss functions\n",
    "    content_loss_criterion = nn.MSELoss()\n",
    "    adversarial_loss_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Move to default device\n",
    "    generator = generator.to(device)\n",
    "    discriminator = discriminator.to(device)\n",
    "    truncated_vgg19 = truncated_vgg19.to(device)\n",
    "    content_loss_criterion = content_loss_criterion.to(device)\n",
    "    adversarial_loss_criterion = adversarial_loss_criterion.to(device)\n",
    "\n",
    "\n",
    "    # Custom dataloaders\n",
    "    working_directory = \"C:\\\\Users\\\\accou\\\\Documents\\\\GitHub\\\\a-PyTorch-Tutorial-to-Super-Resolution_TEMPORAL\\\\\"\n",
    "    folder_path = \"C:\\\\Users\\\\accou\\\\Documents\\\\thesis\\\\data\\\\\"\n",
    "    dataset_file = \"C:\\\\Users\\\\accou\\\\Documents\\\\thesis\\\\images_subfolders3.pkl\"\n",
    "    transform = \"histogram_matching\"\n",
    "    sen2_tile_train = \"T30UXU\"\n",
    "    sen2_tile_test   = \"T30UUU\"\n",
    "    sen2_tile_val  = \"all\"\n",
    "    location = \"local\"\n",
    "    \n",
    "    dataset_train = dataset(folder_path,dataset_file,transform,sen2_amount=4, sen2_tile = sen2_tile_train, location=location)\n",
    "    train_loader = DataLoader(dataset_train,batch_size=batch_size, shuffle=True, num_workers=1,pin_memory=True,drop_last=True)\n",
    "    \n",
    "    dataset_test = dataset(folder_path,dataset_file,transform,sen2_amount=4, sen2_tile = sen2_tile_test, location=location)\n",
    "    test_loader = DataLoader(dataset_test,batch_size=1, shuffle=True, num_workers=1,pin_memory=True,drop_last=True)\n",
    "    \n",
    "    print(\"dataloader instanciated!\")\n",
    "    print(\"Len. Train: \",len(train_loader),\"(Batch Sz. \"+str(batch_size),\")\")\n",
    "    \n",
    "    # Total number of epochs to train for\n",
    "    epochs = int(iterations // len(train_loader) + 1)\n",
    "    epochs = 2000\n",
    "\n",
    "    if logging==True:\n",
    "        run_name = \"SRGAN_temporal_\"+str(datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"))\n",
    "        wandb.init(name=run_name,project=\"SRGAN_temporal\", entity=\"simon-donike\")\n",
    "        wandb.config = {\n",
    "          \"learning_rate\": lr,\n",
    "          \"epochs\": epochs,\n",
    "          \"batch_size\": batch_size}\n",
    "    \n",
    "    \n",
    "    # Epochs\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        wandb.log({'epoch': epoch})\n",
    "        # At the halfway point, reduce learning rate to a tenth\n",
    "        if epoch == int((iterations / 2) // len(train_loader) + 1):\n",
    "            adjust_learning_rate(optimizer_g, 0.1)\n",
    "            adjust_learning_rate(optimizer_d, 0.1)\n",
    "\n",
    "        # One epoch's training\n",
    "        train(train_loader=train_loader,\n",
    "              test_loader=test_loader,\n",
    "              generator=generator,\n",
    "              discriminator=discriminator,\n",
    "              truncated_vgg19=truncated_vgg19,\n",
    "              content_loss_criterion=content_loss_criterion,\n",
    "              adversarial_loss_criterion=adversarial_loss_criterion,\n",
    "              optimizer_g=optimizer_g,\n",
    "              optimizer_d=optimizer_d,\n",
    "              epoch=epoch)\n",
    "        \n",
    "        # log test metrics\n",
    "        test(generator,test_loader)\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'generator': generator,\n",
    "                    'discriminator': discriminator,\n",
    "                    'optimizer_g': optimizer_g,\n",
    "                    'optimizer_d': optimizer_d},\n",
    "                    'checkpoints//checkpoint_SRGAN_'+str(datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\"))+'_epoch'+str(epoch)+'.pth.tar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76abec05-93aa-4cf3-85e2-2a69dc2db4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(generator,test_loader,amount=10):\n",
    "    import utils_.losses as losses\n",
    "    # keep track of error metrics\n",
    "    ssim_ls  = []\n",
    "    psnr_ls  = []\n",
    "    lpips_ls = []\n",
    "    mae_ls = []\n",
    "    ssim_int_ls  = []\n",
    "    psnr_int_ls  = []\n",
    "    lpips_int_ls = []\n",
    "    mae_int_ls = []\n",
    "    \n",
    "    # perform 10 predictions, append results to list\n",
    "    for i in range(0,amount):\n",
    "        lr,hr = next(iter(test_loader))\n",
    "        lr,hr = lr.to(device),hr.to(device)\n",
    "        sr = generator(lr)\n",
    "        l = losses.calculate_metrics(hr,lr,sr)\n",
    "        \n",
    "        \n",
    "        lpips_ls.append(l[0])\n",
    "        psnr_ls.append(l[1])\n",
    "        ssim_ls.append(l[2])\n",
    "        mae_ls.append(l[3])\n",
    "        lpips_int_ls.append(l[4])\n",
    "        psnr_int_ls.append(l[5])\n",
    "        ssim_int_ls.append(l[6])\n",
    "        mae_int_ls.append(l[6])\n",
    "    \n",
    "    def Average(lst):\n",
    "        return sum(lst) / len(lst)\n",
    "    \n",
    "    ssim_avg  = round(Average(ssim_ls),5)\n",
    "    psnr_avg  = round(Average(psnr_ls),5)\n",
    "    lpips_avg = round(Average(lpips_ls),5)\n",
    "    mae_avg = round(Average(mae_ls),5)\n",
    "    ssim_int_avg  = round(Average(ssim_int_ls),5)\n",
    "    psnr_int_avg  = round(Average(psnr_int_ls),5)\n",
    "    lpips_int_avg = round(Average(lpips_int_ls),5)\n",
    "    mae_int_avg = round(Average(mae_int_ls),5)\n",
    "    \n",
    "    wandb.log({'epoch_test_psnr': psnr_avg,\n",
    "               'epoch_test_ssim': ssim_avg,\n",
    "               'epoch_test_lpips': lpips_avg,\n",
    "               'epoch_test_mae': mae_avg})\n",
    "    \n",
    "    #return([lpips_avg, psnr_avg, ssim_avg, mae_avg, lpips_int_avg, psnr_int_avg, ssim_int_avg, mae_int_avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10f3c56-c7b3-4a44-bb39-e2de8b408145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vram():\n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    print(f'total    : {info.total*0.000000001}')\n",
    "    print(f'free     : {info.free*0.000000001}')\n",
    "    print(f'used     : {info.used*0.000000001}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32511e21-6a44-4175-8f5c-47cf6a0659cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,test_loader, generator, discriminator, truncated_vgg19, content_loss_criterion, adversarial_loss_criterion,\n",
    "          optimizer_g, optimizer_d, epoch):\n",
    "    \"\"\"\n",
    "    One epoch's training.\n",
    "\n",
    "    :param train_loader: train dataloader\n",
    "    :param generator: generator\n",
    "    :param discriminator: discriminator\n",
    "    :param truncated_vgg19: truncated VGG19 network\n",
    "    :param content_loss_criterion: content loss function (Mean Squared-Error loss)\n",
    "    :param adversarial_loss_criterion: adversarial loss function (Binary Cross-Entropy loss)\n",
    "    :param optimizer_g: optimizer for the generator\n",
    "    :param optimizer_d: optimizer for the discriminator\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "    # Set to train mode\n",
    "    generator.train()\n",
    "    discriminator.train()  # training mode enables batch normalization\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses_c = AverageMeter()  # content loss\n",
    "    losses_a = AverageMeter()  # adversarial loss in the generator\n",
    "    losses_d = AverageMeter()  # adversarial loss in the discriminator\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    \n",
    "    # Batches\n",
    "    for i, (lr_imgs, hr_imgs) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to default device\n",
    "        lr_imgs = lr_imgs.to(device)  # (batch_size (N), 3,  75,  75), imagenet-normed\n",
    "        hr_imgs = hr_imgs.to(device)  # (batch_size (N), 3, 300, 300), imagenet-normed\n",
    "\n",
    "        \n",
    "        # GENERATOR UPDATE\n",
    "        \n",
    "        # EATS about 4gb VRAM\n",
    "        # Generate\n",
    "        sr_imgs = generator(lr_imgs)  # (N, 3, 300, 300), in [-1, 1]\n",
    "        sr_imgs = convert_image(sr_imgs, source='[-1, 1]', target='imagenet-norm')  # (N, 3, 96, 96), imagenet-normed\n",
    "        \n",
    "        \n",
    "        # EATS 6gb VRAM\n",
    "        # Calculate VGG feature maps for the super-resolved (SR) and high resolution (HR) images\n",
    "        sr_imgs_in_vgg_space = truncated_vgg19(sr_imgs)\n",
    "        hr_imgs_in_vgg_space = truncated_vgg19(hr_imgs).detach()  # detached because they're constant, targets\n",
    "        \n",
    "\n",
    "        # Calculate the Perceptual loss\n",
    "        content_loss = content_loss_criterion(sr_imgs_in_vgg_space, hr_imgs_in_vgg_space)\n",
    "        del sr_imgs_in_vgg_space,hr_imgs_in_vgg_space\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        # EATS like 4gb VRAM\n",
    "        # Discriminate super-resolved (SR) images\n",
    "        sr_discriminated = discriminator(sr_imgs)  # (N)\n",
    "        adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.ones_like(sr_discriminated))\n",
    "\n",
    "        \n",
    "        perceptual_loss = content_loss + beta * adversarial_loss\n",
    "        wandb.log({'content_loss': content_loss,\n",
    "                   'adversarial_loss':adversarial_loss,\n",
    "                   'perceptual_loss':perceptual_loss})\n",
    "    \n",
    "        # Back-prop.\n",
    "        optimizer_g.zero_grad()\n",
    "        perceptual_loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer_g, grad_clip)\n",
    "\n",
    "        # Update generator\n",
    "        optimizer_g.step()\n",
    "\n",
    "        # Keep track of loss\n",
    "        losses_c.update(content_loss.item(), lr_imgs.size(0))\n",
    "        losses_a.update(adversarial_loss.item(), lr_imgs.size(0))\n",
    "\n",
    "        # DISCRIMINATOR UPDATE\n",
    "        # Discriminate super-resolution (SR) and high-resolution (HR) images\n",
    "        hr_discriminated = discriminator(hr_imgs)\n",
    "        sr_discriminated = discriminator(sr_imgs.detach())\n",
    "\n",
    "        \n",
    "        wandb.log({'discr_hr': float(torch.mean(hr_discriminated).item()),\n",
    "                   'discr_sr': float(torch.mean(sr_discriminated).item())})\n",
    "\n",
    "        # But didn't we already discriminate the SR images earlier, before updating the generator (G)? Why not just use that here?\n",
    "        # Because, if we used that, we'd be back-propagating (finding gradients) over the G too when backward() is called\n",
    "        # It's actually faster to detach the SR images from the G and forward-prop again, than to back-prop. over the G unnecessarily\n",
    "        # See FAQ section in the tutorial\n",
    "\n",
    "        # Binary Cross-Entropy loss\n",
    "        adversarial_loss = adversarial_loss_criterion(sr_discriminated, torch.zeros_like(sr_discriminated)) + adversarial_loss_criterion(hr_discriminated, torch.ones_like(hr_discriminated))\n",
    "        del sr_discriminated, hr_discriminated\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Back-prop.\n",
    "        optimizer_d.zero_grad()\n",
    "        adversarial_loss.backward()\n",
    "\n",
    "        # Clip gradients, if necessary\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(optimizer_d, grad_clip)\n",
    "\n",
    "        # Update discriminator\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Keep track of loss\n",
    "        losses_d.update(adversarial_loss.item(), hr_imgs.size(0))\n",
    "\n",
    "        # Keep track of batch times\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        # Reset start time\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            lr_imgs = lr_imgs[0][0:3].unsqueeze(0) # extract 3 bands of first (closest) sen2 image, bring back into batch dimensions\n",
    "            helper_functions.plot_tensors_window(hr_imgs,lr_imgs,sr_imgs,fig_path=\"C:\\\\Users\\\\accou\\\\Documents\\\\GitHub\\\\a-PyTorch-Tutorial-to-Super-Resolution_TEMPORAL\\\\images\\\\\")\n",
    "            print('Epoch: [{0}][{1}/{2}]----'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})----'\n",
    "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})----'\n",
    "                  'Cont. Loss {loss_c.val:.4f} ({loss_c.avg:.4f})----'\n",
    "                  'Adv. Loss {loss_a.val:.4f} ({loss_a.avg:.4f})----'\n",
    "                  'Disc. Loss {loss_d.val:.4f} ({loss_d.avg:.4f})'.format(epoch,\n",
    "                                                                          i,\n",
    "                                                                          len(train_loader),\n",
    "                                                                          batch_time=batch_time,\n",
    "                                                                          data_time=data_time,\n",
    "                                                                          loss_c=losses_c,\n",
    "                                                                          loss_a=losses_a,\n",
    "                                                                          loss_d=losses_d))\n",
    "\n",
    "    del lr_imgs, hr_imgs, sr_imgs   # free some memory since their histories may be stored\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4096588-2a29-4af0-8467-165ffe3fef08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader instanciated!\n",
      "Len. Train:  1531 (Batch Sz. 6 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: simon-donike (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\accou\\AppData\\Local\\Temp\\ipykernel_15316\\176407887.py 71 main\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15316\\3832242952.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15316\\176407887.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mrun_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"SRGAN_temporal_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%d-%m-%Y_%H-%M-%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mwandb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"SRGAN_temporal\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"simon-donike\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         wandb.config = {\n\u001b[0;32m     73\u001b[0m           \u001b[1;34m\"learning_rate\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\geo_env3\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1015\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mlogger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"interrupted\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m         \u001b[0merror_seen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\geo_env3\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[0mexcept_exit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    994\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 995\u001b[1;33m             \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    996\u001b[0m             \u001b[0mexcept_exit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\geo_env3\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mrun_obj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate_run_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_global_run_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\geo_env3\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py\u001b[0m in \u001b[0;36mcommunicate_run_start\u001b[1;34m(self, run_pb)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mrun_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunStartRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mrun_start\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_pb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate_run_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\geo_env3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\u001b[0m in \u001b[0;36m_communicate_run_start\u001b[1;34m(self, run_start)\u001b[0m\n\u001b[0;32m    438\u001b[0m     ) -> Optional[pb.RunStartResponse]:\n\u001b[0;32m    439\u001b[0m         \u001b[0mrec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\geo_env3\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, rec, timeout, local)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     ) -> Optional[pb.Result]:\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_communicate_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mMessageFuture\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\geo_env3\\lib\\site-packages\\wandb\\sdk\\interface\\router.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pb.Result\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mis_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object_ready\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_set\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\geo_env3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\geo_env3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486ae6a-8cc1-4ba1-8d97-33224ace0625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041405f-24cb-4e41-b735-2c4df60a6a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
